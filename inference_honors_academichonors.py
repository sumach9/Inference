# -*- coding: utf-8 -*-
"""inference_honors_academichonors.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jdF47ia942PEynK20RU7Sr7Rh05Nmql2
"""

"""
Academic Honors and Honors Program Inference Pipeline
-----------------------------------------------------------
Author(s): Aakriti Sharma, Ramkumari Maharjan
Date: 04-28-2025
Version: 2.0
Compatible with OpenAI >=1.0.0
"""

# ------------------------------
# Setup: Install & Import Packages
# ------------------------------
# !pip install --upgrade openai
# !pip install python-dateutil

from openai import OpenAI
import json
import os
from dateutil.parser import parse
from datetime import datetime
from typing import List, Dict, Any

# ------------------------------
# OpenAI API Setup
# ------------------------------
client = OpenAI(
    api_key="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"  # <<< replace here
)

# ------------------------------
# Example Input Data
# ------------------------------
example_certification = {
  "person_of_interest": {
    "last_name": {
      "value": "Smithereens",
      "comparison_result": "MATCH"
    },
    "first_name": {
      "value": "John",
      "comparison_result": "MATCH"
    },
    "education": [
      {
        "school_name": {
          "value": "University of Virginia",
          "comparison_result": "MATCH"
        },
        "degree_title": {
          "value": "Master of Science",
          "comparison_result": "MATCH_TBD"
        },
        "dates_of_attendance": {
          "start_date": {
            "value": "July 31, 2020",
            "comparison_result": "MATCH"
          },
          "end_date": {
            "value": "April 30, 2024",
            "comparison_result": "MATCH"
          }
        },
        "date_of_degree_awarded": {
          "value": "2015-05-31",
          "comparison_result": "MATCH_NA"
        },
        "major": [
          {
            "value": "Engineering",
            "comparison_result": "MATCH"
          },
          {
            "value": "Mathematics & Statistics",
            "comparison_result": "MATCH"
          },
          {
            "value": "Physical Sciences",
            "comparison_result": "MATCH_TBD"
          },
          {
            "value": "Science Technologies",
            "comparison_result": "MATCH"
          }
        ],
        "minor": [
          {
            "value": "Computer Science",
            "comparison_result": "MATCH"
          },
          {
            "value": "Philosophy",
            "comparison_result": "MATCH"
          },
          {
            "value": "Mathematics",
            "comparison_result": "MATCH"
          },
          {
            "value": "LINGUISTICS",
            "comparison_result": "MATCH_NA"
          }
        ],
        "academic_honors": [
          {
            "value": "Magna Cum Laude",
            "comparison_result": "MATCH"
          }
        ],
        "honors_program": {
          "value": "1988 Academic Award",
          "comparison_result": "MATCH"
        }
      },
      {
        "school_name": {
          "value": "VS - Hometown University",
          "comparison_result": "MATCH"
        },
        "degree_title": {
          "value": "Bachelor of Science",
          "comparison_result": "MATCH"
        },
        "dates_of_attendance": {
          "start_date": {
            "value": "July 31, 2011",
            "comparison_result": "MATCH"
          },
          "end_date": {
            "value": "April 30, 2015",
            "comparison_result": "MATCH"
          }
        },
        "date_of_degree_awarded": {
          "value": "2015-05-31",
          "comparison_result": "MATCH_NA"
        },
        "major": [
          {
            "value": "Mathematics",
            "comparison_result": "MATCH_TBD"
          }
        ],
        "minor": [
          {
            "value": "Physics",
            "comparison_result": "MATCH_TBD"
          }
        ],
        "academic_honors": [
          {
            "value": "",
            "comparison_result": "MATCH_NA"
          }
        ],
        "honors_program": {
          "value": "",
          "comparison_result": "MATCH_NA"
        }
      },
      {
        "school_name": {
          "value": "George Mason University",
          "comparison_result": "MATCH"
        },
        "degree_title": {
          "value": "Associates Degree",
          "comparison_result": "MATCH"
        },
        "dates_of_attendance": {
          "start_date": {
            "value": "Sept 1, 2013",
            "comparison_result": "MATCH_TBD"
          },
          "end_date": {
            "value": "May 30, 2015",
            "comparison_result": "MATCH_TBD"
          }
        },
        "date_of_degree_awarded": {
          "value": "2015-05-31",
          "comparison_result": "MATCH_NA"
        },
        "major": [
          {
            "value": "Computer Science",
            "comparison_result": "MATCH_TBD"
          }
        ],
        "minor": [
          {
            "value": "",
            "comparison_result": "MATCH_NA"
          }
        ],
        "academic_honors": [
          {
            "value": "Honors Program",
            "comparison_result": "MATCH_TBD"
          }
        ],
        "honors_program": {
          "value": "Associate Scholar",
          "comparison_result": "MATCH_TBD"
        }
      }
    ]
  }}

# ------------------------------
# Prestige Multipliers Map
# ------------------------------
prestige_map = {
    "summa cum laude": (1.2, "Top 5%"),
    "magna cum laude": (1.1, "Top 10–15%"),
    "cum laude": (1.0, "Top 20–30%"),
    "deans list": (0.9, "Top 30–40%"),
    "honors program": (0.8, "Top 10–20% (entry)"),
    "associate scholar": (0.7, "Institution-specific")
}

# ------------------------------
# Helper Functions
# ------------------------------

# Create prompt
def create_prompt(field_type, school, year, value):
    if field_type == "academic_honor":
        return f"Did {school} offer the academic honor '{value}' in {year}? Return ONLY a number between 0 and 1."
    elif field_type == "honors_program":
        return f"Did {school} offer the honors program '{value}' in {year}? Return ONLY a number between 0 and 1."
    return ""

# Query OpenAI
def query_openai(prompt):
    try:
        response = client.chat.completions.create(
            model="gpt-4o",  # or "gpt-4"
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=5
        )
        content = response.choices[0].message.content.strip()
        score = float(content)
        return min(max(score, 0.0), 1.0)
    except Exception as e:
        print(f"OpenAI Query Error: {e}")
        return 0.0

# Annotate each honor field
def verify_and_annotate_honors(certification_data: Dict[str, Any]):
    for record in certification_data.get("person_of_interest", {}).get("education", []):
        school = record.get("school_name", {}).get("value", "Unknown")
        start_date = record.get("dates_of_attendance", {}).get("start_date", {}).get("value", "")
        try:
            year = parse(start_date, fuzzy=True).year
        except Exception:
            continue

        # Academic honors
        honors = record.get("academic_honors", [])
        for honor in honors:
            if honor.get("comparison_result") in ("MATCH_TBD", "MATCH", "NO_MATCH", "MATCH_NA"):
                if honor.get("value", "").strip():
                    prompt = create_prompt("academic_honor", school, str(year), honor.get("value"))
                    score = query_openai(prompt)
                    multiplier, granting_rate = prestige_map.get(honor.get("value", "").lower(), (1.0, "Unknown"))

                    honor["confidence_score"] = score
                    honor["prestige_multiplier"] = multiplier
                    honor["granting_rate"] = granting_rate
                    honor["weighted_score"] = round(score * multiplier, 4)

        # Honors program
        honors_program = record.get("honors_program", {})
        if honors_program.get("comparison_result") in ("MATCH_TBD", "MATCH", "NO_MATCH", "MATCH_NA"):
            if honors_program.get("value", "").strip():
                prompt = create_prompt("honors_program", school, str(year), honors_program.get("value"))
                score = query_openai(prompt)
                multiplier, granting_rate = prestige_map.get(honors_program.get("value", "").lower(), (1.0, "Unknown"))

                honors_program["confidence_score"] = score
                honors_program["prestige_multiplier"] = multiplier
                honors_program["granting_rate"] = granting_rate
                honors_program["weighted_score"] = round(score * multiplier, 4)

    return certification_data

# Compute final weighted score
def compute_final_score_from_verified(certification_data: Dict[str, Any]):
    G = 0.0
    H = 0.0
    for record in certification_data.get("person_of_interest", {}).get("education", []):
        for honor in record.get("academic_honors", []):
            if "weighted_score" in honor:
                multiplier = honor.get("prestige_multiplier", 1.0)
                weighted_score = honor.get("weighted_score", 0.0)
                weight = 5
                G += weighted_score * weight
                H += weight * multiplier

        honors_program = record.get("honors_program", {})
        if "weighted_score" in honors_program:
            multiplier = honors_program.get("prestige_multiplier", 1.0)
            weighted_score = honors_program.get("weighted_score", 0.0)
            weight = 5
            G += weighted_score * weight
            H += weight * multiplier

    return round(G / H, 4) if H > 0 else 0.0

# Master pipeline function
def full_verification_pipeline(certification_data: Dict[str, Any]):
    """
    Executes full honors verification pipeline.
    Returns:
        final_score (float)
        enriched_certification_data (Dict)
    """
    enriched_data = verify_and_annotate_honors(certification_data)
    final_score = compute_final_score_from_verified(enriched_data)
    return final_score, enriched_data

# ------------------------------
# Execution
# ------------------------------

# Run full pipeline
final_score, enriched_certification = full_verification_pipeline(example_certification)

# Print results
print(f" Final Weighted Verification Score: {final_score}")
print(json.dumps(enriched_certification, indent=2))
