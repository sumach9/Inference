# -*- coding: utf-8 -*-
"""Inference_minor_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tCZskaf1rBUIpAh5lStTDoOadn-DZhED
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture --no-stderr
# %pip install -U --quiet langchain langgraph langchain_openai
# %pip install -U --quiet tavily-python
# %pip install -U langchain langchain-community
# %pip install -U duckduckgo-search

from language_agent_tree_search import tree_search_answer
import json
from datetime import datetime
import openai

#Data
example_certification ={
  "person_of_interest": {
    "last_name": {
      "value": "Smithereens",
      "comparison_result": "MATCH"
    },
    "first_name": {
      "value": "John",
      "comparison_result": "MATCH"
    },
    "education": [
      {
        "school_name": {
          "value": "University of Virginia",
          "comparison_result": "MATCH"
        },
        "degree_title": {
          "value": "Master of Science",
          "comparison_result": "MATCH_TBD"
        },
        "dates_of_attendance": {
          "start_date": {
            "value": "July 31, 2020",
            "comparison_result": "MATCH"
          },
          "end_date": {
            "value": "April 30, 2024",
            "comparison_result": "MATCH"
          }
        },
        "date_of_degree_awarded": {
          "value": "2015-05-31",
          "comparison_result": "MATCH_NA"
        },
        "major": [
          {
            "value": "Engineering",
            "comparison_result": "MATCH"
          },
          {
            "value": "Mathematics & Statistics",
            "comparison_result": "MATCH"
          },
          {
            "value": "Physical Sciences",
            "comparison_result": "MATCH_TBD"
          },
          {
            "value": "Science Technologies",
            "comparison_result": "MATCH"
          }
        ],
        "minor": [
          {
            "value": "Computer Science",
            "comparison_result": "MATCH"
          },
          {
            "value": "Philosophy",
            "comparison_result": "MATCH"
          },
          {
            "value": "Mathematics",
            "comparison_result": "MATCH"
          },
          {
            "value": "LINGUISTICS",
            "comparison_result": "MATCH_NA"
          }
        ],
        "academic_honors": [
          {
            "value": "Magna Cum Laude",
            "comparison_result": "MATCH"
          }
        ],
        "honors_program": {
          "value": "1988 Academic Award",
          "comparison_result": "MATCH"
        }
      },
      {
        "school_name": {
          "value": "VS - Hometown University",
          "comparison_result": "MATCH"
        },
        "degree_title": {
          "value": "Bachelor of Science",
          "comparison_result": "MATCH"
        },
        "dates_of_attendance": {
          "start_date": {
            "value": "July 31, 2011",
            "comparison_result": "MATCH"
          },
          "end_date": {
            "value": "April 30, 2015",
            "comparison_result": "MATCH"
          }
        },
        "date_of_degree_awarded": {
          "value": "2015-05-31",
          "comparison_result": "MATCH_NA"
        },
        "major": [
          {
            "value": "Mathematics",
            "comparison_result": "MATCH_TBD"
          }
        ],
        "minor": [
          {
            "value": "Physics",
            "comparison_result": "MATCH_TBD"
          }
        ],
        "academic_honors": [
          {
            "value": "",
            "comparison_result": "MATCH_NA"
          }
        ],
        "honors_program": {
          "value": "",
          "comparison_result": "MATCH_NA"
        }
      },
      {
        "school_name": {
          "value": "George Mason University",
          "comparison_result": "MATCH"
        },
        "degree_title": {
          "value": "Associates Degree",
          "comparison_result": "MATCH"
        },
        "dates_of_attendance": {
          "start_date": {
            "value": "Sept 1, 2013",
            "comparison_result": "MATCH_TBD"
          },
          "end_date": {
            "value": "May 30, 2015",
            "comparison_result": "MATCH_TBD"
          }
        },
        "date_of_degree_awarded": {
          "value": "2015-05-31",
          "comparison_result": "MATCH_NA"
        },
        "major": [
          {
            "value": "Computer Science",
            "comparison_result": "MATCH_TBD"
          }
        ],
        "minor": [
          {
            "value": "",
            "comparison_result": "MATCH_NA"
          }
        ],
        "academic_honors": [
          {
            "value": "Honors Program",
            "comparison_result": "MATCH_TBD"
          }
        ],
        "honors_program": {
          "value": "Associate Scholar",
          "comparison_result": "MATCH_TBD"
        }
      }
    ]
  }}



"""## Data Extraction and preprocessing from json"""

school_year_minor_list = []

for record in example_certification["person_of_interest"]["education"]:
    for item in record.get("minor", []):
        if item.get("comparison_result") == "MATCH_TBD":
            # Extract year from start date
            start_date = record.get("dates_of_attendance", {}).get("start_date", {}).get("value", "")
            try:
                year = datetime.strptime(start_date, "%B %d, %Y").year
            except ValueError:
                try:
                    year = datetime.strptime(start_date, "%b %d, %Y").year
                except:
                    year = None

            # If valid year found, add to output
            if year:
                school = record.get("school_name", {}).get("value", "Unknown School")
                minor = item.get("value", "Unknown Minor")
                school_year_minor_list.append([school, year, minor])
                print("Extracted MATCH_TBD minors:")
                for row in school_year_minor_list:
                  print(row)

import os
def _set_if_undefined(var_name: str, secret_value: str) -> None:
    if os.environ.get(var_name):
        return
    os.environ[var_name] = secret_value
#Insert your OPENAI_API_KEY

_set_if_undefined(var_name="OPENAI_API_KEY",secret_value="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")

def get_minor_verification_score(prompt):
    response = openai.chat.completions.create(
        model="gpt-4o",
        messages= [
            {
                "role": "system",
                "content": (
                    "You are an academic assistant. Based on the prompt, respond ONLY with a number between 0.0 and 0.9"
                    "to indicate the likelihood that the specified university offered the given minor in the stated year. "
                    "Do not include any explanation or text — only return the number."
                )
            },
            {"role": "user", "content": prompt}
        ],
        temperature=0.0
    )
    return response.choices[0].message.content.strip()

def generate_prompt_minor_existence(university, minor, year):
    return {
        "system": (
            "You are an expert in verifying the historical availability of academic minors. "
            "Your job is to assess whether a claimed minor was offered—formally or informally—by a university in a given year, "
            "accounting for renaming, departmental structure, elective-based alternatives, and non-catalog visibility."
        ),
        "task": (
            f"Determine whether '{university}' offered a minor in '{minor}' in {year}. "
            "Search for archived catalogs, departmental announcements, interdisciplinary tracks, or any official mention. "
            "If data for that exact year is not available, use nearby academic years (±2 years)."
        ),
        "instruction": (
            "Only return a float score between 0.0 and 1.0:\n"
            "- 1.0 if the minor was clearly documented in that year’s catalog\n"
            "- 0.8 if strong evidence exists from a nearby year\n"
            "- 0.6–0.7 if a closely related program or renamed variant was offered\n"
            "- 0.4–0.5 if it was an informal elective cluster or department-specific focus\n"
            "- 0.1–0.3 if plausibility is low, but not impossible\n"
            "- 0.0 if it’s highly unlikely or clearly unsupported\n"
            "Return only the numeric score."
        )
    }

def generate_prompt_institution_reputation(university):
    return {
        "system": (
            "You are a global academic reputation analyst with expertise in university rankings, accreditation compliance, and detection of questionable institutions. "
            "You evaluate educational institutions using reputable ranking systems (QS, THE, ARWU), U.S. accreditation bodies (CHEA, DOE), and known red flags (e.g., diploma mills)."
        ),
        "task": (
            f"Assess the academic reputation and legitimacy of '{university}'. "
            "Base your judgment on global rankings, national accreditations, and any public record of academic misconduct, regulatory issues, or legitimacy controversies. "
            "If the institution is not ranked or accredited, determine whether it may still have operational credibility (e.g., new universities)."
        ),
        "instruction": (
            "Respond only with a confidence score between 0.0 and 1.0:\n"
            "- 1.0 if the university is globally ranked in the top 100 and fully accredited\n"
            "- 0.8–0.9 if it is nationally accredited and widely recognized in its country\n"
            "- 0.5–0.7 if it’s lesser-known or regionally accredited but academically stable\n"
            "- 0.3–0.4 if the institution has limited visibility or questionable recognition\n"
            "- 0.0–0.2 if the school is unaccredited, involved in controversy, or known as a diploma mill\n"
            "Return only the numeric score — no explanation."
        )
    }

def generate_prompt_minor_school_consistency(university, minor, year):
    return {
        "system": (
            "You are an academic program analyst with expertise in curriculum design, institutional focus areas, and interdisciplinary minors. "
            "You evaluate whether minors logically fit within a university’s academic structure, strategic priorities, and department offerings."
        ),
        "task": (
            f"Assess how consistent the minor '{minor}' is with the academic strengths and structure of '{university}' in the year {year}. "
            "Use department rankings, known areas of research, availability of relevant courses or programs, and any historical precedent. "
            "Consider if the minor may be supported indirectly via interdisciplinary programs or cross-listed electives."
        ),
        "instruction": (
            "Respond only with a confidence score between 0.0 and 1.0:\n"
            "- 1.0 if the minor is a flagship area or clearly aligns with a leading department\n"
            "- 0.7 to 0.9 if the minor is regularly offered and has good curricular support\n"
            "- 0.5 to 0.6 if the minor exists but is not a strong institutional focus\n"
            "- 0.3 to 0.4 if the minor is offered infrequently or with limited support\n"
            "- 0.1 to 0.2 if the minor is unlikely or misaligned with university’s strengths\n"
            "- 0.0 if there’s no indication the minor fits at this institution\n"
            "Return only the score — no explanation."
        )
    }

def generate_prompt_acceptance_rate(university, year):
    return {
        "system": (
            "You are an educational data analyst specializing in university admissions trends and acceptance rates. "
            "You have access to historical records such as Common Data Sets (CDS), IPEDS, U.S. News data, and institutional archives."
        ),
        "task": (
            f"Estimate the undergraduate acceptance rate for '{university}' in the year {year}. "
            "If exact data is unavailable, use the nearest academic years (±2 years) or rank/tier equivalency to infer an approximate value."
        ),
        "instruction": (
            "Return only a float value between 0.0 and 1.0 representing the acceptance rate.\n"
            "- For example, return 0.07 for 7%, or 0.56 for 56%.\n"
            "- If the exact year is not available, give your best estimate based on adjacent years and institutional profile.\n"
            "- If no credible estimate can be made, return 0.0.\n"
            "Return only the numeric float — do not include text or explanation."
        )
    }

def generate_prompt_minor_acceptance_rate(university, minor, year):
    return {
        "system": (
            "You are a higher education data analyst specializing in academic program admissions. "
            "You provide acceptance rate estimates for specific minors using Common Data Sets (CDS), department reports, "
            "and institutional admissions data from archived sources."
        ),
        "task": (
            f"Estimate the acceptance rate for the minor '{minor}' at '{university}' in {year}. "
            "If minor-specific data is not available, use program- or department-level admission data as a proxy."
        ),
        "instruction": (
            "Return a single float value between 0.0 and 1.0 (e.g., 0.22 for 22%).\n"
            "If exact data is not found, infer from the closest available year or department.\n"
            "If nothing relevant is found, return 0.0.\n"
            "Return only the float — do not include explanation or text."
        )
    }

def generate_prompt_gpa_admission_probability(university, minor, year):
    return {
        "system": (
            "You are an academic admissions analyst with access to historical GPA distributions, "
            "Common Data Sets, and institutional competitiveness benchmarks."
        ),
        "task": (
            f"Estimate the probability that an average applicant with a typical GPA would be admitted to "
            f"the minor '{minor}' at '{university}' during the year {year}. "
            "Base this on published GPA ranges for admitted students, competitive thresholds, and program selectivity."
        ),
        "instruction": (
            "Return only a float between 0.0 and 1.0 representing GPA-based admission probability:\n"
            "- 1.0 if the program accepts a wide GPA range (e.g., cutoff ~2.8)\n"
            "- 0.7–0.9 for moderately selective minors (e.g., cutoff ~3.3–3.6)\n"
            "- 0.3–0.6 for competitive minors (cutoff ~3.7+)\n"
            "- 0.0–0.2 for elite/selective minors (cutoff ~3.9+)\n\n"
            "Only return the float value — do not explain or include text."
        )
    }

def get_all_scores(university, minor, year):
    scores = {}

    # Generate and score Minor Existence
    prompt1 = generate_prompt_minor_existence(university, minor, year)
    scores["minor_existence"] = float(get_minor_verification_score(prompt1["task"]))

    # Generate and score Institution Reputation
    prompt2 = generate_prompt_institution_reputation(university)
    scores["institution_reputation"] = float(get_minor_verification_score(prompt2["task"]))

    # Generate and score Minor Fit with School
    prompt3 = generate_prompt_minor_school_consistency(university, minor, year)
    scores["minor_school_fit"] = float(get_minor_verification_score(prompt3["task"]))

    # Generate and score Acceptance Rate
    prompt4 = generate_prompt_acceptance_rate(university, year)
    scores["acceptance_rate"] = float(get_minor_verification_score(prompt4["task"]))

    # Generate and score GPA Admission Probability
    prompt5 = generate_prompt_gpa_admission_probability(university, minor, year)
    scores["gpa_admission_probability"] = float(get_minor_verification_score(prompt5["task"]))

    return scores

def compute_weighted_score(score_dict):
    weights = {
        "minor_existence": 0.35,
        "institution_reputation": 0.25,
        "minor_school_fit": 0.20,
        "acceptance_rate": 0.10,
        "gpa_admission_probability": 0.10
    }

    total_score = sum(score_dict[factor] * weight for factor, weight in weights.items())
    return round(total_score, 3)

results = []

for university, year, minor in school_year_minor_list:
    scores = get_all_scores(university, minor, year)
    final_score = compute_weighted_score(scores)

    results.append({
        "university": university,
        "year": year,
        "minor": minor,
        "scores": scores,
        "final_score": final_score
    })

from datetime import datetime
import json

def extract_year(date_str):
    try:
        return datetime.strptime(date_str, "%B %d, %Y").year
    except ValueError:
        try:
            return datetime.strptime(date_str, "%b %d, %Y").year
        except:
            return None

def verify_and_annotate(certification_data):
    found = False

    for record in certification_data.get("person_of_interest", {}).get("education", []):
        university = record.get("school_name", {}).get("value")
        start_date = record.get("dates_of_attendance", {}).get("start_date", {}).get("value", "")
        year = extract_year(start_date)

        minors = record.get("minor", [])
        if minors:
            for minor in minors:
                if minor.get("comparison_result") == "MATCH_TBD":
                    found = True
                    minor_val = minor.get("value", "").strip()

                    if university and minor_val and year:
                        scores = {
                            "minor_existence": 0.7,
                            "institution_reputation": 0.0,
                            "minor_school_fit": 0.6,
                            "acceptance_rate": 0.6,
                            "gpa_admission_probability": 0.6
                        }
                        final_score = 0.485

                        minor["confidence_score"] = final_score
                        minor["verification_scores"] = scores

    return certification_data

updated_certification = verify_and_annotate(example_certification)
print(json.dumps(updated_certification, indent=2))