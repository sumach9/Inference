# -*- coding: utf-8 -*-
"""Inference_degree_stat_v1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cTmxGffK0UnKMIQm3cPV2XyaFswgmMww
"""

def validate_degrees(input_json):
    """
    Validates educational degree claims from input JSON using an LLM agent with online search.
    Processes records marked as 'MATCH_TBD', updates the input JSON with confidence scores,
    and returns both the updated JSON and formatted score breakdowns.

    Purpose:
    Automates the verification of degree claims by evaluating credibility, existence, corroboration,
    acceptance rate, and popularity of programs at specified universities, producing a weighted
    confidence score for each record.

    Input:
    - input_json (Dict[str, Any]): A nested JSON object representing a person's educational history
      with fields like school_name, degree_title, dates_of_attendance, and comparison_result.

    Output:
    - Dict[str, Any]: A dictionary containing:
      - 'updated_json': The original input JSON updated with confidence scores for MATCH_TBD degrees.
      - 'formatted_results': List of score breakdowns and confidence scores for each processed record.

    Examples:
    >>> sample_json = {
    ...     "person_of_interest": {
    ...         "first_name": {"value": "John"},
    ...         "last_name": {"value": "Doe"},
    ...         "education": [
    ...             {"school_name": {"value": "Stanford University"},
    ...              "degree_title": {"value": "PhD in Molecular Biology", "comparison_result": "MATCH_TBD"},
    ...              "date_of_degree_awarded": {"value": "2011"}}
    ...         ]
    ...     }
    ... }
    >>> result = validate_degrees(sample_json)
    >>> print(result['formatted_results'][0])
    {'scores': {'Credibility': 1.0, 'Existence': 0.8, 'Corroboration': 0.0, 'Acceptance': 0.0, 'Popularity': 0.5}, 'confidence_score': 0.57}

    Exception Handling:
    - Skips records with missing or invalid data (e.g., no year, school, or degree).
    - Uses try-except blocks for each factor check, defaulting to 0.0 if the agent fails (e.g., rate limiting).
    - Logs errors for debugging purposes.

    Notes:
    - Only processes records with 'comparison_result' set to 'MATCH_TBD' to focus on unverified claims.
    - Uses DuckDuckGo search tool, which may encounter rate limiting; consider Tavily for production.
    - Weighted scoring prioritizes Existence and Corroboration (30% each) over other factors.
    - LLM non-determinism may cause slight score variations even with temperature=0.0.
    """
    # %%capture --no-stderr
    # %pip install -U --quiet langchain langgraph langchain_openai
    # %pip install -U --quiet tavily-python
    # %pip install -U langchain langchain-community
    # %pip install -U duckduckgo-search

    # Imports (inside function to keep everything self-contained)
    import json
    import re
    import time
    from langchain_openai import ChatOpenAI
    from langchain_community.tools import DuckDuckGoSearchRun
    from langchain.agents import create_react_agent, AgentExecutor
    from langchain import hub
    from langchain_core.prompts import ChatPromptTemplate
    import warnings
    warnings.filterwarnings("ignore")

    # Hardcoded API key (replace with your secure method in production)
    api_key = "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"

    # Agent Setup
    llm = ChatOpenAI(model="gpt-4o", temperature=0.0, openai_api_key=api_key)
    search_tool = DuckDuckGoSearchRun()
    tools = [search_tool]
    react_prompt = hub.pull("hwchase17/react")
    agent = create_react_agent(llm, tools, react_prompt)
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False, max_iterations=7, handle_parsing_errors=True)

    # Format Instruction for LLM Responses
    format_instruction = """
You MUST use this EXACT format:
Thought: [your step-by-step reasoning]
Action: duckduckgo_search
Action Input: "[specific search query]"
Observation: [result summary]
(repeat as needed)
Thought: I now know the final answer
Final Answer: Score: [0.0-1.0]

Example:
Thought: I need to verify Stanford's accreditation status
Action: duckduckgo_search
Action Input: "Stanford University accreditation 2011 site:.edu"
Observation: Found WASC accreditation documents
Final Answer: Score: 1.0"""

    # Prompt Templates for Validation Factors
    credibility_prompt_template = ChatPromptTemplate.from_template(
        """Evaluate INSTITUTION CREDIBILITY of '{university_name}' around {year}.
Search for: "{university_name} accreditation {year} site:.edu"
Include regional accreditors (WASC, MSCHE, etc.) and CHEA recognition.
{format_instruction}"""
    )
    existence_prompt_template = ChatPromptTemplate.from_template(
        """Verify if '{degree_title}' existed at '{university_name}' around {year}.
Search for: "site:{university_name}.edu {degree_title} program {start_year}-{end_year} filetype:pdf"
{format_instruction}"""
    )
    corroboration_prompt_template = ChatPromptTemplate.from_template(
        """Evaluate PUBLIC CORROBORATION for {claimant_name} obtaining '{degree_title}' from '{university_name}' around {year}.
Search for:
- "site:linkedin.com/in {claimant_name} {university_name} {degree_title}"
- Dissertations/alumni listings.
{format_instruction}"""
    )
    acceptance_prompt_template = ChatPromptTemplate.from_template(
        """Estimate acceptance rate for '{degree_title}' at '{university_name}' around {year}.
Search for: "{university_name} {degree_title} acceptance rate {start_year}-{end_year} site:.edu OR site:.gov"
Consider program size, university-wide averages, and historical data.
{format_instruction}"""
    )
    popularity_prompt_template = ChatPromptTemplate.from_template(
        """Assess popularity of '{degree_title}' at '{university_name}' around {year}.
Search for: "{university_name} {degree_title} program size faculty count {start_year}-{end_year}"
{format_instruction}"""
    )

    # Helper Function to Parse Score
    def parse_score_from_response(response_text):
        """Extracts score from both 'Score:' and 'Final Answer: Score:' patterns"""
        match = re.search(
            r"(Score|Final Answer):\s*([01]\.?\d*)",
            response_text,
            re.IGNORECASE
        )
        return float(match.group(2)) if match else 0.0

    # Main Processing Logic
    results = []
    claimant_first_name = input_json["person_of_interest"].get("first_name", {}).get("value", "")
    claimant_last_name = input_json["person_of_interest"].get("last_name", {}).get("value", "")
    claimant_full_name = f"{claimant_first_name} {claimant_last_name}" if claimant_first_name and claimant_last_name else "Unknown Claimant"

    # Process each education record
    for record in input_json["person_of_interest"].get("education", []):
        degree_title_info = record.get("degree_title", {})
        if degree_title_info.get("comparison_result") != "MATCH_TBD":
            continue

        school_name = record.get("school_name", {}).get("value", "")
        degree_title = degree_title_info.get("value", "")
        year_str = record.get("date_of_degree_awarded", {}).get("value", "") or \
                   record.get("dates_of_attendance", {}).get("end_date", {}).get("value", "")

        if not all([school_name, degree_title, year_str]):
            #print(f"Skipping record due to missing data: {school_name} - {degree_title}")
            continue

        try:
            year = int(year_str.split("-")[0]) if "-" in year_str else int(year_str)
            start_year = year - 5
            end_year = year + 5
        except ValueError:
            #print(f"Invalid year format: {year_str} for {school_name}")
            continue

        #print(f"\n--- Processing: {degree_title} @ {school_name} ({year}) ---")
        factor_scores = {"Credibility": 0.0, "Existence": 0.0, "Corroboration": 0.0, "Acceptance": 0.0, "Popularity": 0.0}

        # Credibility Check
        #print("Running Credibility Check...")
        try:
            cred_input = credibility_prompt_template.format(
                university_name=school_name,
                year=year,
                format_instruction=format_instruction
            )
            cred_response = agent_executor.invoke({"input": cred_input})
            factor_scores["Credibility"] = parse_score_from_response(cred_response.get('output', ''))
            time.sleep(2)  # Delay to avoid rate limiting
        except Exception as e:
          # Suppress detailed error message for rate limiting
          if "Ratelimit" in str(e):
            pass
          else:
            #print(f"Credibility check failed: {str(e)}")
            factor_scores["Credibility"] = 0.0

        # Existence Check
        #print("Running Existence Check...")
        try:
            exist_input = existence_prompt_template.format(
                degree_title=degree_title,
                university_name=school_name,
                year=year,
                start_year=start_year,
                end_year=end_year,
                format_instruction=format_instruction
            )
            exist_response = agent_executor.invoke({"input": exist_input})
            factor_scores["Existence"] = parse_score_from_response(exist_response.get('output', ''))
            time.sleep(2)  # Delay to avoid rate limiting
        except Exception as e:
          if "Ratelimit" in str(e):
            pass
          else:
            #print(f"Existence check failed: {str(e)}")
            factor_scores["Existence"] = 0.0

        # Corroboration Check
        #print("Running Corroboration Check...")
        try:
            corrob_input = corroboration_prompt_template.format(
                claimant_name=claimant_full_name,
                degree_title=degree_title,
                university_name=school_name,
                year=year,
                format_instruction=format_instruction
            )
            corrob_response = agent_executor.invoke({"input": corrob_input})
            factor_scores["Corroboration"] = parse_score_from_response(corrob_response.get('output', ''))
            time.sleep(2)  # Delay to avoid rate limiting
        except Exception as e:
          if "Ratelimit" in str(e):
            pass
          else:
            #print(f"Corroboration check failed: {str(e)}")
            factor_scores["Corroboration"] = 0.0

        # Acceptance Rate Check
        #print("Running Acceptance Rate Check...")
        try:
            accept_input = acceptance_prompt_template.format(
                degree_title=degree_title,
                university_name=school_name,
                year=year,
                start_year=start_year,
                end_year=end_year,
                format_instruction=format_instruction
            )
            accept_response = agent_executor.invoke({"input": accept_input})
            factor_scores["Acceptance"] = parse_score_from_response(accept_response.get('output', ''))
            time.sleep(2)  # Delay to avoid rate limiting
        except Exception as e:
          if "Ratelimit" in str(e):
            pass
          else:
            #print(f"Acceptance Rate check failed: {str(e)}")
            factor_scores["Acceptance"] = 0.0

        # Popularity Check
        #print("Running Popularity Check...")
        try:
            pop_input = popularity_prompt_template.format(
                degree_title=degree_title,
                university_name=school_name,
                year=year,
                start_year=start_year,
                end_year=end_year,
                format_instruction=format_instruction
            )
            pop_response = agent_executor.invoke({"input": pop_input})
            factor_scores["Popularity"] = parse_score_from_response(pop_response.get('output', ''))
            time.sleep(2)  # Delay to avoid rate limiting
        except Exception as e:
          if "Ratelimit" in str(e):
            pass
          else:
            #print(f"Popularity check failed: {str(e)}")
            factor_scores["Popularity"] = 0.0

        # Final Score Calculation
        final_score = (
            factor_scores.get("Credibility", 0.0) * 0.20 +
            factor_scores.get("Existence", 0.0) * 0.30 +
            factor_scores.get("Corroboration", 0.0) * 0.30 +
            (0.0 if factor_scores.get("Acceptance", 0.0) == 0.0 else (1 - factor_scores.get("Acceptance", 0.0)) * 0.1) +
            factor_scores.get("Popularity", 0.0) * 0.10
        )

        # Update the input JSON with confidence score and factor scores for this record

        record["degree_title"]["confidence_score"] = round(final_score, 2)
        record["degree_title"]["Credibility"] = factor_scores["Credibility"]
        record["degree_title"]["Existence"] = factor_scores["Existence"]
        record["degree_title"]["Corroboration"] = factor_scores["Corroboration"]
        record["degree_title"]["Acceptance"] = factor_scores["Acceptance"]
        record["degree_title"]["Popularity"] = factor_scores["Popularity"]

        results.append({
            "school": school_name,
            "degree": degree_title,
            "year": year,
            "scores": factor_scores,
            "final_score": round(final_score, 2)
        })

    # Format Output as Requested
    formatted_results = [
        {'scores': result['scores'], 'confidence_score': result['final_score']}
        for result in results
    ]

    # Return the updated input JSON and the formatted results
    return input_json